# Optimizing an ML Pipeline in Azure

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

## Summary
**In 1-2 sentences, explain the problem statement: e.g "This dataset contains data about... we seek to predict..."**

The dataset used in this project seems to be derived from https://archive.ics.uci.edu/ml/datasets/bank+marketing, which includes client data related to a bank's marketing campaign. The classification goal is to predict if the client will subscribe (yes/no) a term deposit (variable y), based on 20 attributes (either numeric or categorical) of client.

**In 1-2 sentences, explain the solution: e.g. "The best performing model was a ..."**

The best performing model (0.9717) is VotingEnsemble that was obtained using Azure Auto ML.
It performed slightly better than Sklearn Logistic regression (0.9616) and using Azure Hyperdrive for hyperparameter tuning.

## Scikit-learn Pipeline
**Explain the pipeline architecture, including data, hyperparameter tuning, and classification algorithm.**

Pipeline includes 
1. Read data and cleanup:
Most categorical variables are converted to numeric format (using one hot encoding, or binary values 1/0, or mapped values for months/days)
2. Split the dataset into train & validation set
3. Training a model using train split and "Logistic regression" classification algorithm, which is a linear model.  Validation set was used to evaluate model performace. Azure Hyperdrive was used to tune following model hyperparameters: 
- The regularization strength (C) that determines amount of model's overfitting to training data set.
- Max number of iterations (max_iter) allowed for model training / convergence 
4. Save the best performing model & register it for deployment.

**What are the benefits of the parameter sampler you chose?**

In random sampling that is used here, hyperparameter values for C & max_iter are randomly selected from the defined search space. It avoids user bias of choosing specific discrete hyperparameters and also is more efficient than grid search (which could have much larger search space) 
Ref: https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.hyperdrive.randomparametersampling?preserve-view=true&view=azure-ml-py

**What are the benefits of the early stopping policy you chose?**

Early stopping policy (Bandit policy in this case) terminates runs which seem as potentially low performing models. Any run that doesn't fall within the slack factor or slack amount of the evaluation metric with respect to the best performing run will be terminated. It therefore increase the computational efficiency of training. However, in this case since we are logging the metric "Accuracy" only after the entire model is trained in train.py, it may not have much impact. This would be much more useful in a Deep learning model scenario where the callbacks log the "Accuracy" for each batch.
Ref: https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.hyperdrive.banditpolicy?preserve-view=true&view=azure-ml-py#&preserve-view=truedefinition

## AutoML
**In 1-2 sentences, describe the model and hyperparameters generated by AutoML.**

Automated machine learning tries different models and algorithms during the automation and tuning process. As a user, there is no need for you to specify the algorithm. The final high performing model from AutoML is VotingEnsemble, which is an ensemble of many child run models with soft-voting (i.e. weighted averages of their predictions). 

## Pipeline comparison
**Compare the two models and their performance. What are the differences in accuracy? In architecture? If there was a difference, why do you think there was one?**

Although the difference in the accuracy is not significant, Auto ML gives slightly better accuracy.
AutoML also uses slightly more data than Hyperdrive since we are using 5 fold cross-validation.
But AutoML is more convenient from user's point of view, since it allows us to explore many more models and avoids user bias in terms of hyperparameter ranges to explore.
The final high performing model from AutoML (VotingEnsemble) is an ensemble of many models, compared to a single Logistic regression model that Hyperdrive used.

## Future work
**What are some areas of improvement for future experiments? Why might these improvements help the model?**

Few improvements that come to mind are:
-  More feature engineering could be done - Month & day of week are currently directly mapped to numerical values ; They should ideally be one hot encoded (or set using learned embeddings) since we are setting relative importance.
-  Can increase the timeout for AutoML experiments or number of Hyperdrive iterations - so that more model / hyperparameters can be explored
-  Use a better metric (like AUC / precision*) to evaluate models, since accuracy is not a good metric for datasets with imbalanced output classes 
-  Evaluation should be done on a test set instead of validation set - since hyperparameter tuning is done on validation set & the model could be overfit to this distribution
-  Use Azure Pipelines to code the steps, so that pipeline can be visualized in ML studio & can be automated better

--- Remove ---
## Proof of cluster clean up
**If you did not delete your compute cluster in the code, please complete this section. Otherwise, delete this section.**
**Image of cluster marked for deletion**
